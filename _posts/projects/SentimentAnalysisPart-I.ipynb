{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model for Sentiment Analysis Using Natural Language Processing  Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "**In this post, I will be building and evaluating a text classification model for sentiment analysis. In part I, I will learn from the data by loading all the data into the computer's internal memory while in part II, I will be exploring out-of-core learning learning techniques to allow learning on large data sets when there are memory limitations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the data\n",
    "** The training data used for this analysis is the 1.6 million English language Stanford twitter corpus that has been automatically annotated for negative and positive sentiment using emoticons. The test data is also from the Stanford group and consists of tweets that have been manually annotated with 177 reflecting negative sentiment and 182 reflecting positive sentiment. The data sets can be found here: http://help.sentiment140.com/for-students**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re #for regex\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**In the loadData function, we will load the data from csv to a Pandas dataframe do some shuffling so as to get a mix of all sentiments, and remove unecessary columns. We will then pickle the data to disk to allow for faster loading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    \"\"\"\n",
    "    Load the cvs files with the training and test data\n",
    "    \"\"\"\n",
    "    header = ['polarity', 'tweet_id', 'date','query', 'user', 'tweet']\n",
    "    \n",
    "    df_train = pd.read_csv('/home/concinte/Code/SentimentAnalysis/Twitter/\\\n",
    "    Data/training.1600000.processed.noemoticon.csv',\n",
    "                 header=None)\n",
    "    df_test = pd.read_csv('/home/concinte/Code/SentimentAnalysis/Twitter/\\\n",
    "    Data/testdata.manual.2009.06.14.csv',\n",
    "                 header=None)\n",
    "\n",
    "    df_train.columns = header\n",
    "    df_test.columns  = header\n",
    "\n",
    "    #Shuffle the rows so that you get a mix of pos, neutral, and neg sentiments\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test  = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #Drop unnecessary columns\n",
    "    df_train.drop(['tweet_id','date','query','user'], axis=1, inplace=True)\n",
    "    df_test.drop(['tweet_id','date','query','user'], axis=1, inplace=True)\n",
    "\n",
    "    #Pickle the data frames\n",
    "    df_train.to_pickle('/Data/df_training.pkl')\n",
    "    df_test.to_pickle('/Data/df_test.pkl')\n",
    "    \n",
    "    print \"Finished loading and pickling data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**The preProcessTweet function takes a tweet and does some pre-processing on the tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessTweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to pre-process the tweet\n",
    "    \"\"\"\n",
    "    #str(tweet.encode('utf-8')) \n",
    "    str(tweet)\n",
    "    \n",
    "    #Replace all words preceded by '@' with 'USER_NAME'\n",
    "    tweet = re.sub(r'@[^\\s]+', 'USER_NAME', tweet)\n",
    "    \n",
    "    #Replace all url's with 'URL'\n",
    "    tweet = re.sub(r'www.[^\\s]+ | http[^\\s]+',' URL ', tweet)\n",
    "    \n",
    "    #Replace all hashtags with the word\n",
    "    tweet = tweet.strip('#')\n",
    "    \n",
    "    #Replace words with long repeated characters with the shorter form\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet)\n",
    "    \n",
    "    #Remove any extra white space\n",
    "    tweet = re.sub(r'[\\s]+', ' ', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**In preProcessData, we take the previoulsy pickled dataframes, pre-process each tweet in the dataframe, and pickle the now pre-processed dataframes for faster loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessData():\n",
    "    \"\"\"\n",
    "    Obtained the pickled data and pre-process.\n",
    "    The pre-processed data is then pickled \n",
    "    \"\"\"\n",
    "    df_train = pd.read_pickle('/Data/df_training.pkl')\n",
    "    df_test = pd.read_pickle('/Data/df_test.pkl')\n",
    "    \n",
    "    #Pre-process the data\n",
    "    df_train['tweet'] = df_train['tweet'].apply(preProcessTweet)\n",
    "    df_test['tweet'] = df_test['tweet'].apply(preProcessTweet)\n",
    "    \n",
    "    #Pickle pre-processed data frames\n",
    "    df_train.to_pickle('/Data/df_training_preprocessed.pkl')\n",
    "    df_test.to_pickle('/Data/df_test_preprocessed.pkl')\n",
    "\n",
    "    print \"Training and test data is now pre-processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**The feature_Extractor functions takes a tweet as argument and extracts the features in the tweet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_Extractor(tweet):\n",
    "    \"\"\"\n",
    "    Takes a tweet and extracts its features\n",
    "    \"\"\"\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**getFeatureVector will take a tweet as argument and tokenize each tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet):\n",
    "    \"\"\"\n",
    "    The function takes a tweet and does some processing\n",
    "    to remove stopwords, remove punctuation, lemmatize/stem\n",
    "    and reject any words that are non-alpha. Depending on the \n",
    "    flag selected, it will return a unigram, bigram, or a\n",
    "    mix of the two. It returns a list with the filtered n-grams\n",
    "    \"\"\"\n",
    "    \n",
    "    flag = 3 #1 for unigram; 2 for bigram; 3 for mix\n",
    "    \n",
    "    #tokenize the tweet and convert each token to lower case\n",
    "    #tokens = [token.lower() for token in word_tokenize(tweet)]\n",
    "    tokens = [token.lower() for token in word_tokenize(tweet.decode('latin-1'))]\n",
    "\n",
    "    punctuations = [\"'\", \":\", \",\", \"-\", \".\", \"!\", \"(\", \")\", \"?\", '\"', \";\"]\n",
    "    stopWords = stopwords.words('english')\n",
    "    stopWords.append(\"#\")\n",
    "    stopWords.append(\"%\")\n",
    "    stopWords = set(stopWords)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #Remove stopwords, punctuation, 'url', and 'user_name'\n",
    "    filteredTokens = []\n",
    "    featureVector = []\n",
    "    for token in tokens:\n",
    "        if (token in punctuations or token in stopWords):\n",
    "               continue\n",
    "        elif (token == 'url' or token == 'user_name'):\n",
    "            continue\n",
    "        elif token.isalpha()== False: #reject non-alpha tokens\n",
    "            continue\n",
    "        else:\n",
    "            #Normalize the tokens, either by stemming or lemmatization\n",
    "            #I might also have to tag the tokens with Parts of Speech\n",
    "            #<lemmatize words>\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            \n",
    "            #This is the feature vector for each tweet\n",
    "            filteredTokens.append(token)\n",
    "            if flag == 1:\n",
    "                #unigrams\n",
    "                featureVector = filteredTokens\n",
    "            elif flag == 2:\n",
    "                #bigrams \n",
    "                featureVector = list(nltk.bigrams(filteredTokens))\n",
    "                if featureVector != []: #ensure it is not an empty list\n",
    "                    #Convert the tuple of bigrams to a string\n",
    "                    featureVector = [' '.join(bigram) for bigram in featureVector]\n",
    "            else:\n",
    "                #mixgrams\n",
    "                featureVector = list(nltk.everygrams(filteredTokens, max_len=2))\n",
    "                if featureVector != []:\n",
    "                    #Convert any tuple of n-grams to a string\n",
    "                    temp = []\n",
    "                    for everygram in featureVector:\n",
    "                        if type(everygram) == tuple:\n",
    "                            everygram = ' '.join(everygram)\n",
    "                        temp.append(everygram)\n",
    "                    featureVector = temp\n",
    "                                         \n",
    "    return featureVector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(df):\n",
    "    \"\"\"\n",
    "    This function obtains features from a data set using a Bag of Words or Bag of n-grams approach\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    allWords = []\n",
    "    \n",
    "    #Set flag for unigram (Bag of words) or n-gram(Bag of n-grams)\n",
    "    #Flag = 2 # 1 is unigram; 2 is bigram; any otehr is a mixed bag of unigram and bigram (everygram)\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        polarity = row[1]\n",
    "        tweet = row[2]\n",
    "        \n",
    "        #Obtain the feature vector for each tweet\n",
    "        featureVector = getFeatureVector(tweet)\n",
    "        \n",
    "        #tweets is a list containing tuples of filtered n-grams\n",
    "        #and their respective sentiments\n",
    "        tweets.append((featureVector, polarity))\n",
    "        \n",
    "        #Get list of all words/n-grams from all the tweets\n",
    "        allWords.extend(featureVector)\n",
    "            \n",
    "    #Return dict with the frequency distribution of each word/n-gram\n",
    "    wordDist = nltk.FreqDist(allWords)\n",
    "    \n",
    "    #Get a list of the features with each word/n-gram in the dist as a feature\n",
    "    featureList = wordDist.keys()\n",
    "        \n",
    "    return featureList, tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTrainFeatureList(num):\n",
    "    \n",
    "    #Load pre-processed pickled data frame\n",
    "    df = pd.read_pickle('/Data/df_training_preprocessed.pkl')\n",
    "    df = df[:num]\n",
    "    \n",
    "    #Extract the data set\n",
    "    featureList, tweets = getFeatures(df)\n",
    "    \n",
    "    #Pickle the feature list and tweets\n",
    "    num = num/1000\n",
    "    pickle.dump(featureList, open('featureList_train_{0}k.pkl'.format(num), 'wb'))\n",
    "    pickle.dump(tweets, open('tweets_train_{0}k.pkl'.format(num), 'wb')) \n",
    "    \n",
    "    print \"Pickle of train feature list and {0}k tweets successful\".format(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTestFeatureList():\n",
    "    \n",
    "    #Load pre-processed pickled data frame\n",
    "    df = pd.read_pickle('/Data/df_test_preprocessed.pkl')\n",
    "    \n",
    "    #Drop neutral polarity rows in test data (I don't have a neutral class in my training set)\n",
    "    df = df[df.polarity !=2]\n",
    "    \n",
    "    #Extract the data set\n",
    "    featureList, tweets = getFeatures(df)\n",
    "    \n",
    "    #Pickle the feature list and tweets\n",
    "    pickle.dump(featureList, open('featureList_test.pkl', 'wb'))\n",
    "    pickle.dump(tweets, open('tweets_test.pkl', 'wb')) \n",
    "    \n",
    "    print \"Pickle of test feature list and tweets successful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Pre-process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preProcessData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train feature list and tweets list. generateFeatureList takes the number of tweets to train on as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle of train feature list and 10k tweets successful\n"
     ]
    }
   ],
   "source": [
    "generateTrainFeatureList(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate test feature list and tweets list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle of test feature list and tweets successful\n"
     ]
    }
   ],
   "source": [
    "generateTestFeatureList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureList = pickle.load(open('featureList_train_10k.pkl', 'rb')) \n",
    "tweets = pickle.load(open('tweets_train_10k.pkl', 'rb')) \n",
    "training_set = nltk.classify.apply_features(feature_Extractor, tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and pickle the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NBclassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "pickle.dump(NBclassifier, open('NBclassifier_10K.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a quick test of the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testtweet = \"I love turtles\"\n",
    "processedTweet = preProcessTweet(testtweet)\n",
    "feature_vec = getFeatureVector(processedTweet)\n",
    "features =  feature_Extractor(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "if NBclassifier.classify(features)==0:\n",
    "    print \"Negative\"\n",
    "else:\n",
    "    print \"Positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureList = pickle.load(open('featureList_test.pkl', 'rb')) \n",
    "tweets = pickle.load(open('tweets_test.pkl', 'rb')) \n",
    "testing_set = nltk.classify.apply_features(feature_Extractor, tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load and evaluate the pickled Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('NBclassifier_10K.pkl', 'rb')\n",
    "NBclassifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy is 74.02 %:\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(NBclassifier, testing_set )*100\n",
    "print(\"Classification accuracy is %.2f %%:\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Show the most valuable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(sad) = True                0 : 4      =     24.8 : 1.0\n",
      "          contains(died) = True                0 : 4      =     17.1 : 1.0\n",
      "       contains(welcome) = True                4 : 0      =     15.5 : 1.0\n",
      "       contains(anymore) = True                0 : 4      =     13.5 : 1.0\n",
      "          contains(poor) = True                0 : 4      =     13.4 : 1.0\n",
      "          contains(sick) = True                0 : 4      =     13.2 : 1.0\n",
      "       contains(ca find) = True                0 : 4      =     13.1 : 1.0\n",
      "          contains(hurt) = True                0 : 4      =     12.1 : 1.0\n",
      "         contains(shame) = True                0 : 4      =     11.7 : 1.0\n",
      "      contains(headache) = True                0 : 4      =     11.6 : 1.0\n",
      "         contains(upset) = True                0 : 4      =     10.4 : 1.0\n",
      "        contains(lonely) = True                0 : 4      =     10.4 : 1.0\n",
      "    contains(wish could) = True                0 : 4      =     10.2 : 1.0\n",
      "           contains(ugh) = True                0 : 4      =      9.8 : 1.0\n",
      "        contains(throat) = True                0 : 4      =      9.7 : 1.0\n",
      "            contains(bo) = True                0 : 4      =      9.7 : 1.0\n",
      "    contains(sorry hear) = True                0 : 4      =      9.7 : 1.0\n",
      "           contains(wtf) = True                0 : 4      =      9.7 : 1.0\n",
      "    contains(first time) = True                4 : 0      =      9.6 : 1.0\n",
      "       contains(go away) = True                0 : 4      =      9.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "NBclassifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the last column shows the number of times a certain feature appears in one class as compared to the other. So, for example, \"sad\" appears 24.8 more times in a negative tweet as compared to a positive one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "### <font color='Navy'>Final words on this analysis</font>\n",
    "**<font color='Navy'>In this analysis, I found that I was limited on the size of the dataset I could use for training due to the need to load the full dataset to memory. In the next part, I will explore out-of-core techniques for text classification.**</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

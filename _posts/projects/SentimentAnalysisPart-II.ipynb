{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model for Sentiment Analysis Using Natural Language Processing  Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a previous post, I demonstrated how one can go about building a model for sentiment analysis using text analysis and natural language processing or NLP. In that post, I found that due to memory limitations, I was only able to train on 20000 tweets out of a tweet corpus comprising of 1.6 million tweets. In this analysis, I will be exploring out-of-core techniques for classification of text documents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the data\n",
    "** The training data used for this analysis is the 1.6 million English language Stanford twitter corpus that has been automatically annotated for negative and positive sentiment using emoticons. The test data is also from the Stanford group and consists of tweets that have been manually annotated with 177 reflecting negative sentiment and 182 reflecting positive sentiment. The data sets can be found here: http://help.sentiment140.com/for-students**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re #for regex\n",
    "import cPickle as pickle\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Get the raw data and do some data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    \"\"\"\n",
    "    Load the cvs files with the training and test data\n",
    "    \"\"\"\n",
    "    header = ['polarity', 'tweet_id', 'date','query', 'user', 'tweet']\n",
    "    \n",
    "    df_train = pd.read_csv('/home/concinte/Code/SentimentAnalysis/Twitter/\\\n",
    "    Data/training.1600000.processed.noemoticon.csv',\n",
    "                 header=None)\n",
    "    df_test = pd.read_csv('/home/concinte/Code/SentimentAnalysis/Twitter/\\\n",
    "    Data/testdata.manual.2009.06.14.csv',\n",
    "                 header=None)\n",
    "\n",
    "    df_train.columns = header\n",
    "    df_test.columns  = header\n",
    "\n",
    "    #Shuffle the rows so that you get a mix of pos, neutral, and neg sentiments\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test  = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #Drop unnecessary columns\n",
    "    df_train.drop(['tweet_id','date','query','user'], axis=1, inplace=True)\n",
    "    df_test.drop(['tweet_id','date','query','user'], axis=1, inplace=True)\n",
    "\n",
    "    #Pickle the data frames\n",
    "    df_train.to_pickle('/Data/df_training.pkl')\n",
    "    df_test.to_pickle('/Data/df_test.pkl')\n",
    "    \n",
    "    print \"Finished loading and pickling data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessTweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to pre-process the tweet\n",
    "    \"\"\"\n",
    "    str(tweet)\n",
    "    \n",
    "    #Replace all words preceded by '@' with 'USER_NAME'\n",
    "    tweet = re.sub(r'@[^\\s]+', 'USER_NAME', tweet)\n",
    "    \n",
    "    #Replace all url's with 'URL'\n",
    "    tweet = re.sub(r'www.[^\\s]+ | http[^\\s]+ | https[^\\s]+',' URL ', tweet)\n",
    "    \n",
    "    #Replace all hashtags with the word\n",
    "    tweet = tweet.strip('#')\n",
    "    \n",
    "    #Replace words with long repeated char with the shorter form\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet)\n",
    "    \n",
    "    #Remove any extra white space\n",
    "    tweet = re.sub(r'[\\s]+', ' ', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessData():\n",
    "    \"\"\"\n",
    "    Obtained the pickled data and pre-process.\n",
    "    The pre-processed data is then pickled \n",
    "    \"\"\"\n",
    "    df_train = pd.read_pickle('/Data/df_training.pkl')\n",
    "    df_test = pd.read_pickle('/Data/df_test.pkl')\n",
    "    \n",
    "    #Pre-process the data\n",
    "    df_train['tweet'] = df_train['tweet'].apply(preProcessTweet)\n",
    "    df_test['tweet'] = df_test['tweet'].apply(preProcessTweet)\n",
    "    \n",
    "    #Pickle pre-processed data frames\n",
    "    df_train.to_pickle('/Data/df_training_preprocessed.pkl')\n",
    "    df_test.to_pickle('/Data/df_test_preprocessed.pkl')\n",
    "\n",
    "    print \"Training and test data is now pre-processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Implementation of the out-of-core technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pre-processed pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Data/df_training_preprocessed.pkl')\n",
    "df = df[:800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('/Data/df_test_preprocessed.pkl')\n",
    "df_test = df_test[df_test.polarity !=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweetGenerator(df):\n",
    "    \"\"\"\n",
    "    This function takes a tweet from the data frame\n",
    "    and returns the text of the tweet as well as the sentiment label\n",
    "    \"\"\"\n",
    "    for row in df.itertuples():\n",
    "        label = row[1]\n",
    "        tweet = row[2]\n",
    "        yield tweet, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getFeatureVector.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getFeatureVector.py\n",
    "def getFeatureVector(tweet):\n",
    "    \"\"\"\n",
    "    The function takes a tweet and does some processing\n",
    "    to remove stopwords, remove punctuation, lemmatize/stem\n",
    "    and reject any words that are non-alpha. Depending on the \n",
    "    flag selected, it will return a unigram, bigram, or a\n",
    "    mix of the two. It returns a list with the filtered n-grams\n",
    "    \"\"\"\n",
    "    \n",
    "    flag = 3 #1 for unigram; 2 for bigram; 3 for mix\n",
    "    \n",
    "    #Tokenize the tweet and convert each token to lower case\n",
    "    tokens = [token.lower() for token in word_tokenize(tweet)]\n",
    "\n",
    "    punctuations = [\"'\", \":\", \",\", \"-\", \".\", \"!\", \"(\", \")\", \"?\", '\"', \";\"]\n",
    "    stopWords = stopwords.words('english')\n",
    "    stopWords.append(\"#\")\n",
    "    stopWords.append(\"%\")\n",
    "    stopWords = set(stopWords)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #Remove stopwords, punctuation, 'url', and 'user_name'\n",
    "    filteredTokens = []\n",
    "    featureVector = []\n",
    "    for token in tokens:\n",
    "        if (token in punctuations or token in stopWords):\n",
    "               continue\n",
    "        elif (token == 'url' or token == 'user_name'):\n",
    "            continue\n",
    "        elif token.isalpha()== False: #Reject non-alpha tokens\n",
    "            continue\n",
    "        else:\n",
    "            #Lemmatize the tokens\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            \n",
    "            #This is the feature vector for each tweet\n",
    "            filteredTokens.append(token)\n",
    "            if flag == 1:\n",
    "                #unigrams\n",
    "                featureVector = filteredTokens\n",
    "            elif flag == 2:\n",
    "                #bigrams \n",
    "                featureVector = list(nltk.bigrams(filteredTokens))\n",
    "                if featureVector != []: #ensure it is not an empty list\n",
    "                    #Convert the tuple of bigrams to a string\n",
    "                    featureVector = [' '.join(bigram) for bigram in featureVector]\n",
    "            else:\n",
    "                #mixgrams\n",
    "                featureVector = list(nltk.everygrams(filteredTokens, max_len=2))\n",
    "                if featureVector != []:\n",
    "                    #Convert any tuple of n-grams to a string\n",
    "                    temp = []\n",
    "                    for everygram in featureVector:\n",
    "                        if type(everygram) == tuple:\n",
    "                            everygram = ' '.join(everygram)\n",
    "                        temp.append(everygram)\n",
    "                    featureVector = temp\n",
    "                                         \n",
    "    return featureVector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(tweet_gen, size):\n",
    "    \"\"\"\n",
    "    This function takes as arguments\n",
    "    the tweet generator tweet_gen and the batch size\n",
    "    desired. It returns two lists for the tweets and labels\n",
    "    whose length is the batch size\n",
    "    \"\"\"\n",
    "    tweets, labels = [], []\n",
    "    for _ in range(size):\n",
    "        tweet, label = next(tweet_gen)\n",
    "        tweets.append(tweet)\n",
    "        labels.append(label)\n",
    "    return tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatchTest(tweet_gen_test, size):\n",
    "    \"\"\"\n",
    "    This function is used to generate the batches\n",
    "    for the test data\n",
    "    \"\"\"\n",
    "    tweets, labels = [], []\n",
    "    for _ in range(size):\n",
    "        tweet, label = next(tweet_gen_test)\n",
    "        tweets.append(tweet)\n",
    "        labels.append(label)\n",
    "    return tweets, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a HashingVectorizer which takes the tokenizing function as one of its arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vector = HashingVectorizer(decode_error = 'ignore',\n",
    "                          n_features = 2**21,\n",
    "                          preprocessor = None,\n",
    "                          non_negative=True,\n",
    "                          encoding='utf-8',\n",
    "                          tokenizer = getFeatureVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is important that non_negative is set to true. From the scikit-learn homepage:\n",
    ">Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature’s value is zero.\n",
    ">If non_negative=True is passed to the constructor, the absolute value is taken. This undoes some of the collision handling, but allows the output to be passed to estimators like sklearn.naive_bayes.MultinomialNB or sklearn.feature_selection.chi2 feature selectors that expect non-negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNBclassifier = MultinomialNB(alpha = 0.01)\n",
    "tweet_gen = tweetGenerator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BernoulliNBclassifier = BernoulliNB(alpha = 0.01)\n",
    "tweet_gen = tweetGenerator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDclassifier = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "tweet_gen = tweetGenerator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "PassiveAggressiveclassifier = PassiveAggressiveClassifier(C=1.0, fit_intercept=True, n_iter=5, shuffle=True)\n",
    "tweet_gen = tweetGenerator(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyprind is a python function that provides a progress bar for iterative processes. We will iteratively call each classifier for each batch of the training data and monitor the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:18:52\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "batchSize = 20000\n",
    "totalTweets = len(df)\n",
    "#totalTweets = 15000\n",
    "iterations = totalTweets/batchSize\n",
    "progressBar = pyprind.ProgBar(iterations)\n",
    "\n",
    "classes = np.array([0, 4])\n",
    "for i in range(iterations):\n",
    "    X_train, y_train = getBatch(tweet_gen, size=batchSize)\n",
    "    X_train = vector.transform(X_train)\n",
    "    MNBclassifier.partial_fit(X_train, y_train, classes=classes)\n",
    "    BernoulliNBclassifier.partial_fit(X_train, y_train, classes=classes)\n",
    "    SGDclassifier.partial_fit(X_train, y_train, classes=classes)\n",
    "    PassiveAggressiveclassifier.partial_fit(X_train, y_train, classes=classes)\n",
    "    progressBar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and score each classifier using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_gen_test = tweetGenerator(df_test)\n",
    "X_test, y_test = getBatchTest(tweet_gen_test, size=df_test.shape[0])\n",
    "X_test = vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.751\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % MNBclassifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.707\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % BernoulliNBclassifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % SGDclassifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % PassiveAggressiveclassifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pickle the tokenizer, HashingVectorizer, and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "\n",
    "dill.dump(vector,open('./Classifiers/HashingVectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(df)/1000\n",
    "dill.dump(MNBclassifier, open('./Classifiers/MNBclassifier_OutofCore_{0}k.pkl'.format(num), 'wb'))\n",
    "\n",
    "dill.dump(BernoulliNBclassifier, open('./Classifiers/\\\n",
    "BernoulliNBclassifier_OutofCore_{0}k.pkl'.format(num), 'wb'))\n",
    "\n",
    "dill.dump(SGDclassifier, open('./Classifiers/\\\n",
    "SGDclassifier_OutofCore_{0}k.pkl'.format(num), 'wb'))\n",
    "\n",
    "dill.dump(PassiveAggressiveclassifier, open('./Classifiers/\\\n",
    "PassiveAggressiveclassifier_OutofCore_{0}k.pkl'.format(num), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from getFeatureVector import getFeatureVector\n",
    "dill.dump(getFeatureVector, open('./Classifiers/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### <font color='Navy'>Note on improving performance</font>\n",
    "**<font color='Navy'>The performance of the models can be improved by pairing the capabilities of the NLTK suite of models and those of the Scikit-learn library. We can use grid search on each of the algorithms with cross-validation to select for the optimum set of hyperparameters, and then apply cross-validation in the training of the models once the optimum hyperparameters have been selected. A demonstration of this will be at a later date.</font>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
